{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBb0VXpuvgIEdrabB7AA9A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kdats/regulation_of_post_virality/blob/main/collab_filter_for_AI_recommentations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2ejhi3nb8YP",
        "outputId": "d08a4d9a-0145-4339-a59f-78d9e87331e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5714, 30), (5731, 30))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse.linalg import svds\n",
        "import random\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/engagement_data_set.csv'\n",
        "interaction_df = pd.read_csv(file_path)\n",
        "\n",
        "interaction_df.drop(columns =['Unnamed: 0.2', 'Unnamed: 0.1', 'Unnamed: 0'], inplace=True)\n",
        "\n",
        "# Preprocessing: Fill NaN values in 'numbr_likes' and 'number_comments' with 0\n",
        "interaction_df['numbr_likes'] = interaction_df['numbr_likes'].fillna(0)\n",
        "interaction_df['number_comments'] = interaction_df['number_comments'].fillna(0)\n",
        "\n",
        "# Calculate the engagement score as the sum of likes and comments\n",
        "interaction_df['engagement_score'] = interaction_df['numbr_likes'] + interaction_df['number_comments']\n",
        "\n",
        "# drop duplicates\n",
        "interaction_df.drop_duplicates([\"post_id\"] ).shape, interaction_df.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Aggregate engagement metrics for unique combinations of profile_id and post_id\n",
        "interaction_df = interaction_df.groupby(['profile_id', 'post_id'], as_index=False).agg({\n",
        "    'numbr_likes': 'sum',\n",
        "    'number_comments': 'sum'\n",
        "})\n",
        "\n",
        "# Calculate the engagement score as the sum of likes and comments\n",
        "interaction_df['engagement_score'] = interaction_df['numbr_likes'] + interaction_df['number_comments']\n",
        "# Filter out posts and users with minimal engagement\n",
        "interaction_df = interaction_df[interaction_df['engagement_score'] > 0]\n",
        "# Create a user-item matrix with engagement scores\n",
        "user_item_matrix = interaction_df.pivot(index='profile_id', columns='post_id', values='engagement_score').fillna(0)\n",
        "\n",
        "# Check matrix density (percentage of non-zero values)\n",
        "density = np.count_nonzero(user_item_matrix.values) / user_item_matrix.size\n",
        "print(f\"Matrix Density: {density * 100:.2f}%\")\n",
        "\n",
        "# Filter out users or posts with very low interaction to reduce sparsity further, if density is too low\n",
        "min_engagement_threshold = 2  # Minimum engagement threshold\n",
        "user_item_matrix = user_item_matrix.loc[user_item_matrix.sum(axis=1) > min_engagement_threshold]\n",
        "user_item_matrix = user_item_matrix.loc[:, user_item_matrix.sum(axis=0) > min_engagement_threshold]\n",
        "\n",
        "# Ensure matrix is of type float\n",
        "user_item_matrix = user_item_matrix.astype(float)\n",
        "\n",
        "# Perform Singular Value Decomposition (SVD), re-checking for density\n",
        "if np.count_nonzero(user_item_matrix.values) / user_item_matrix.size > 0.01:  # Proceed if density is reasonable\n",
        "    U, sigma, Vt = svds(user_item_matrix, k=5)\n",
        "    sigma = np.diag(sigma)\n",
        "\n",
        "    # Calculate predicted engagement scores\n",
        "    predicted_scores = np.dot(np.dot(U, sigma), Vt)\n",
        "    predicted_df = pd.DataFrame(predicted_scores, columns=user_item_matrix.columns, index=user_item_matrix.index)\n",
        "\n",
        "    # Function to get basic recommendations using collaborative filtering\n",
        "    def recommend_posts(user_id, num_recommendations=5):\n",
        "        if user_id not in predicted_df.index:\n",
        "            return \"User not found in data.\"\n",
        "        user_row = predicted_df.loc[user_id].sort_values(ascending=False)\n",
        "        return user_row.index[:num_recommendations].tolist()\n",
        "\n",
        "    # Function to get regulated recommendations\n",
        "    def regulated_recommend_posts(user_id, num_recommendations=5):\n",
        "        # Basic recommendations from collaborative filtering\n",
        "        recommended_posts = recommend_posts(user_id, num_recommendations * 2)\n",
        "\n",
        "        # Add diversity: sample additional content randomly\n",
        "        all_posts = user_item_matrix.columns.tolist()\n",
        "        additional_posts = random.sample(all_posts, min(num_recommendations, len(all_posts)))\n",
        "\n",
        "        # Balance original recommendations with additional content\n",
        "        balanced_recommendations = recommended_posts[:num_recommendations] + additional_posts\n",
        "\n",
        "        # Apply time-decay weights for recency preference\n",
        "        recency_weights = {post: np.exp(-i/len(balanced_recommendations)) for i, post in enumerate(balanced_recommendations)}\n",
        "        balanced_recommendations = sorted(balanced_recommendations, key=lambda post: recency_weights[post], reverse=True)\n",
        "\n",
        "        # Remove duplicates and limit recommendations\n",
        "        unique_recommendations = []\n",
        "        for post in balanced_recommendations:\n",
        "            if post not in unique_recommendations:\n",
        "                unique_recommendations.append(post)\n",
        "            if len(unique_recommendations) >= num_recommendations:\n",
        "                break\n",
        "\n",
        "        return unique_recommendations\n",
        "\n",
        "    # Example usage: Get recommendations for a specific user_id\n",
        "    user_id_example = interaction_df['profile_id'].iloc[0]  # example user ID\n",
        "    print(\"Basic Recommendations:\", recommend_posts(user_id=user_id_example))\n",
        "    print(\"Regulated Recommendations:\", regulated_recommend_posts(user_id=user_id_example))\n",
        "else:\n",
        "    print(\"Matrix density too low for collaborative filtering.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VqvXe4TcVs5",
        "outputId": "2ea4ddbd-cb23-4e43-8d90-2a509d86964f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix Density: 0.02%\n",
            "Matrix density too low for collaborative filtering.\n"
          ]
        }
      ]
    }
  ]
}